{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azhara001/INFO-259-Natural-Language-Processing-/blob/main/HW1/HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7OZtl39xfQ8"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucbnlp24/hws4nlp24/blob/main/HW1/HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nO4tc2mOoYa",
        "outputId": "f094cdb3-39f2-47f9-a5c6-2872e145875d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J-vYvH4iYtL"
      },
      "source": [
        "# Homework 1: Featurized Models for Sentiment Analysis\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this homework, you will write code that generates features for the positive/negative sentiment classification of movie\n",
        "reviews, and train a model on those features to predict the sentiment of new data. Example of selected movie reviews\n",
        "with labels:\n",
        "\n",
        "| Label | Review\n",
        "| :--- | :------\n",
        "| **pos** | What can I say, it’s a damn good movie. See it if you still haven’t. Great camera works and lighting <br> techniques. Awesome, just awesome.  Orson Welles is incredible ‘The Lady From Shanghai’ can certainly <br> take the place of ‘Citizen Kane’.\n",
        "| **neg** | Can I give this a minus rating? No? Well, let me say that this is the most atrocious film I have ever tried to <br> watch. It was Painful. Boringus Maximus. The plot(?) is well hidden in several sub-levels of <br> nebulosity. I rented this film with a friend and, after about thirty minutes of hoping it would get better, we <br> decided to “fast forward” a little to see if things would get any better. It never gets better. This film about <br> some dude getting kidnapped by these two girls, sounds interesting, but, in reality, it is just a bore. <br> Nothing even remotely interesting ever happens. If you ever get the chance to watch this, do <br> yourself a favor, try “PLAN NINE FROM OUTER SPACE” instead ”  \n",
        "\n",
        "This notebook contains code which performs all the data parsing, model training, and evaluation for you. Once you are in this notebook, **save a copy** in your Google Drive or Github before working on the file.\n",
        "\n",
        "During training, a basic logistic regression model (from the scikit-learn package) is trained with your created features\n",
        "to predict each review’s sentiment. Your ultimate goal for this assignment is to featurize the text creatively and optimize\n",
        "the accuracy of the model on the test data.\n",
        "\n",
        "Your main tasks regarding *classification* are to **implement four features**:\n",
        "1. A bag of words feature. Fill out the **bag_of_words** function in the notebook to represent a document through its bag of words. Represent each **lowercase** word tokenized with the NLTK **word_tokenize** function through its binary value. Remember that the feature value for a word that shows up in a review should be 1, no matter how many times it is mentioned. Please keep your code between the **#BEGIN/#END SOLUTION** flags.\n",
        "2. Create **three** different classes of features. Implement them in the **feature1** **feature2** and **feature3** functions. Create features that you think would perform better than bag of words, and assess their independent performance on the development data. Describe your reasoning in your Colab notebook and include accuracy scores on the dev.txt data (printed in the Colab output). **You must report your reasoning and development scores in the table provided.** It is not required that your features actually perform well, but your justification and reasoning should be defensible. We are looking for thoughtful insights as to why your original features *should* outperform the simple bag of words implementation.\n",
        "\n",
        "For more feature ideas consult [SLP Ch. 5](https://web.stanford.edu/~jurafsky/slp3/5.pdf) and lecture slides, and please do keep the following constraints in mind:\n",
        "\n",
        "* Do not use pre-trained word vectors (including static embeddings like word2vec or contextual embeddings like BERT); we will cover these later.\n",
        "* Do not use the predictions of any other supervised model on this data as a feature for logistic regression (e.g., do not train a separate CNN on this data, make predictions on the dev data using the CNN, and then treat your CNN prediction as a feature).\n",
        "* Do not alter {train,dev,test}.txt or find additional sources of training data. This homework is focusing on how you *represent* your data, not other aspects of the training regime.\n",
        "* Do not import any other additional libraries.\n",
        "\n",
        "After completing classificaion there is a **task** for *evaluation*:\n",
        "1. Fill in the dictionary `param_grid`, to find which L2 regularization strength improves your model the most.\n",
        "\n",
        "The assignment ends with a **reflection**:\n",
        "1. *Short answer* regarding applying assignment concepts to a real world classification evaluation situation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuQg4112iYtM"
      },
      "source": [
        "## How to Submit\n",
        "\n",
        "There are two deliverables to be submitted on Gradescope. Submit to GradeScope [HW 1](https://www.gradescope.com/courses/701350/assignments/3984973):\n",
        "  - Submit `HW1.ipynb`. The name of this file should stay unchanged for the Gradescope autograder.\n",
        "  - Submit `combiner_function_predictions.csv`. The file must be named in this way for the Gradescope autograder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TQTT9x-6d2JI"
      },
      "outputs": [],
      "source": [
        "##################################\n",
        "## Please Do not edit the imports\n",
        "##################################\n",
        "\n",
        "from scipy import sparse\n",
        "from sklearn import linear_model\n",
        "from collections import Counter\n",
        "import operator\n",
        "import nltk\n",
        "import csv\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pandas import option_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4KuVSCSqlUX",
        "outputId": "cfcaf98d-f9c5-41b3-c62c-b3aa0316f8ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "!python -m nltk.downloader punkt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqs2iWGGxfRA"
      },
      "source": [
        "## Intro: Gather Data + Create Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9wUSxBPxfRA"
      },
      "source": [
        "### Gather Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hk07KCgwoZy"
      },
      "source": [
        "Let's download the data we'll use for training and development, and also the data we'll make predictions with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn0XtfFeqP2P",
        "outputId": "7015947e-984a-40eb-dfbb-b7f40c15aac3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-06 21:18:26--  https://raw.githubusercontent.com/ucbnlp24/hws4nlp24/main/HW1/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1322055 (1.3M) [text/plain]\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "\rtrain.txt             0%[                    ]       0  --.-KB/s               \rtrain.txt           100%[===================>]   1.26M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-02-06 21:18:26 (20.3 MB/s) - ‘train.txt’ saved [1322055/1322055]\n",
            "\n",
            "--2024-02-06 21:18:26--  https://raw.githubusercontent.com/ucbnlp24/hws4nlp24/main/HW1/dev.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1309909 (1.2M) [text/plain]\n",
            "Saving to: ‘dev.txt’\n",
            "\n",
            "dev.txt             100%[===================>]   1.25M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-02-06 21:18:26 (22.4 MB/s) - ‘dev.txt’ saved [1309909/1309909]\n",
            "\n",
            "--2024-02-06 21:18:26--  https://raw.githubusercontent.com/ucbnlp24/hws4nlp24/main/HW1/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2651165 (2.5M) [text/plain]\n",
            "Saving to: ‘test.txt’\n",
            "\n",
            "test.txt            100%[===================>]   2.53M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-02-06 21:18:26 (37.2 MB/s) - ‘test.txt’ saved [2651165/2651165]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get data\n",
        "!wget https://raw.githubusercontent.com/ucbnlp24/hws4nlp24/main/HW1/train.txt\n",
        "!wget https://raw.githubusercontent.com/ucbnlp24/hws4nlp24/main/HW1/dev.txt\n",
        "!wget https://raw.githubusercontent.com/ucbnlp24/hws4nlp24/main/HW1/test.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jq2yq0xpRCUb"
      },
      "outputs": [],
      "source": [
        "trainingFile = \"train.txt\"\n",
        "evaluationFile = \"dev.txt\"\n",
        "testFile = \"test.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4K9RsEWxfRB"
      },
      "source": [
        "### Define Classifier class"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eLwwsYU94_cT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffvtjtyDxfRB"
      },
      "source": [
        "Next, we've created a Binary Classifier. This class will let us learn the traits associated with positive and negatively classed movie reviews in order to make predictions on our test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "CGiM8qQiJOBU"
      },
      "outputs": [],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code.\n",
        "## This defines the classification class which\n",
        "## loads the data and sets up the model.\n",
        "######################################################################\n",
        "\n",
        "class Classifier:\n",
        "\n",
        "    def __init__(self, feature_method, L2_regularization_strength=1.0, min_feature_count=1):\n",
        "        # Initialize variables and set up logistic regression model\n",
        "        self.feature_vocab = {} # Dictionary to store feature vocabulary\n",
        "        self.feature_method = feature_method # Method to extract features from text\n",
        "        self.log_reg = linear_model.LogisticRegression() # Logistic Regression model\n",
        "        self.L2_regularization_strength=L2_regularization_strength # L2 regularization strength\n",
        "        self.min_feature_count=min_feature_count # Minimum count for a feature to be considered\n",
        "\n",
        "        # Process training, development, and test datasets\n",
        "        self.trainX, self.trainY, self.trainOrig = self.process(trainingFile, training=True)\n",
        "        self.devX, self.devY, self.devOrig = self.process(evaluationFile, training=False)\n",
        "        self.testX, _, self.testOrig = self.process(testFile, training=False)\n",
        "\n",
        "    # Read data from file and return as a list of tuples\n",
        "    def load_data(self, filename):\n",
        "        data = []\n",
        "        with open(filename, encoding=\"utf8\") as file:\n",
        "            for line in file:\n",
        "                cols = line.split(\"\\t\")\n",
        "                idd = cols[0]\n",
        "                label = cols[1]\n",
        "                text = cols[2]\n",
        "\n",
        "                data.append((idd, label, text))\n",
        "\n",
        "        return data\n",
        "\n",
        "    # Featurize entire dataset using the specified feature extraction method\n",
        "    def featurize(self, data):\n",
        "        featurized_data = []\n",
        "        for idd, label, text in data:\n",
        "            feats = self.feature_method(text)\n",
        "            featurized_data.append((label, feats))\n",
        "        return featurized_data\n",
        "\n",
        "    # Read dataset and returned featurized representation as sparse matrix + label array\n",
        "    def process(self, dataFile, training = False):\n",
        "        original_data = self.load_data(dataFile)\n",
        "        # print(f\"load_data function result: {len(original_data)}\")\n",
        "        # print(f\"first sample point outcome from load_data: {original_data[0]}\\n\")\n",
        "\n",
        "        data = self.featurize(original_data)\n",
        "\n",
        "        # for feature in data:\n",
        "        #   print(len(feature[1]))\n",
        "        # print(f\"post featurize outcome length: {len(data)}\")\n",
        "        # print(f\"first sample point outcome from featurize: {data[0]}\\n\")\n",
        "\n",
        "        # If training, build feature vocabulary and filter features based on minimum count\n",
        "        if training:\n",
        "            fid = 0\n",
        "            feature_doc_count = Counter()\n",
        "            for label, feats in data:\n",
        "                for feat in feats:\n",
        "                    feature_doc_count[feat]+= 1\n",
        "\n",
        "            for feat in feature_doc_count:\n",
        "                if feature_doc_count[feat] >= self.min_feature_count:\n",
        "                    self.feature_vocab[feat] = fid\n",
        "                    fid += 1\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        #print(f\"Sparse Matrix dimensions: {X.shape}\")\n",
        "        Y = [None]*D\n",
        "        for idx, (label, feats) in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "            Y[idx] = label\n",
        "\n",
        "        return X, Y, original_data\n",
        "\n",
        "    def load_test(self, dataFile):\n",
        "        data = self.load_data(dataFile)\n",
        "        data = self.featurize(data)\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        Y = [None]*D\n",
        "        for idx, (data_id, feats) in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "            Y[idx] = data_id\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    # Train model and evaluate on held-out data\n",
        "    def evaluate(self):\n",
        "        (D,F) = self.trainX.shape\n",
        "        self.log_reg = linear_model.LogisticRegression(C = self.L2_regularization_strength, max_iter=1000)\n",
        "        self.log_reg.fit(self.trainX, self.trainY)\n",
        "        training_accuracy = self.log_reg.score(self.trainX, self.trainY)\n",
        "        development_accuracy = self.log_reg.score(self.devX, self.devY)\n",
        "        print(\"Method: %s, Features: %s, Train accuracy: %.3f, Dev accuracy: %.3f\" % (self.feature_method.__name__, F, training_accuracy, development_accuracy))\n",
        "\n",
        "\n",
        "    # Predict labels for new data and write the predictions to a CSV file\n",
        "    def predict(self):\n",
        "        predX = self.log_reg.predict(self.testX)\n",
        "\n",
        "        with open(\"%s_%s\" % (self.feature_method.__name__, \"predictions.csv\"), \"w\", encoding=\"utf8\") as out:\n",
        "            writer=csv.writer(out)\n",
        "            writer.writerow([\"Id\", \"Expected\"])\n",
        "            for idx, data_id in enumerate(self.testX):\n",
        "                writer.writerow([self.testOrig[idx][0], predX[idx]])\n",
        "        out.close()\n",
        "\n",
        "    # Method to print top weights for each feature\n",
        "    def printWeights(self, n=10):\n",
        "\n",
        "        reverse_vocab=[None]*len(self.log_reg.coef_[0])\n",
        "        for k in self.feature_vocab:\n",
        "            reverse_vocab[self.feature_vocab[k]]=k\n",
        "\n",
        "        # binary\n",
        "        if len(self.log_reg.classes_) == 2:\n",
        "              weights=self.log_reg.coef_[0]\n",
        "\n",
        "              cat=self.log_reg.classes_[1]\n",
        "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "              cat=self.log_reg.classes_[0]\n",
        "              for feature, weight in list(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1)))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "        # multiclass\n",
        "        else:\n",
        "          for i, cat in enumerate(self.log_reg.classes_):\n",
        "\n",
        "              weights=self.log_reg.coef_[i]\n",
        "\n",
        "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-yJ5AiSxfRB"
      },
      "source": [
        "## Examples: A Simple Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDmfkG782kgo"
      },
      "source": [
        "Let's create an initial classifier based on a really simple feature using a dictionary:\n",
        "\n",
        "* if the abstract contains the words \"love\" or \"like\", the `contains_positive_word` feature will fire, and\n",
        "* if it contains either \"hate\" or \"dislike\", the `contains_negative_word` will fire.  \n",
        "\n",
        "Note how we use `nltk.word_tokenize` to tokenize the text into its discrete words (the documentation for which can be found [here](https://www.nltk.org/api/nltk.tokenize.html))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xCq1bL3e2jUj"
      },
      "outputs": [],
      "source": [
        "def simple_featurize(text):\n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    for word in words:\n",
        "        word=word.lower()\n",
        "        if word == \"love\" or word == \"like\":\n",
        "            feats[\"contains_positive_word\"] = 1\n",
        "        if word == \"hate\" or word == \"dislike\":\n",
        "            feats[\"contains_negative_word\"] = 1\n",
        "\n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3PQdN9r3Ujz"
      },
      "source": [
        "Now let's see how that feature performs on the development data.\n",
        "\n",
        "Note the `L2_regularization_strength` parameter specifies the strength of the L2 regularizer **(values closer to 0 = stronger regularization)**, and `min_feature_count` specifies how many data points need to contain a feature for it to be passed into the model as a feature. Both are ways to prevent the model from overfitting and achieve higher performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jnqjxd6fKPiP",
        "outputId": "64e426fe-8660-4cf0-ac3a-e5e345cadeeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: simple_featurize, Features: 2, Train accuracy: 0.509, Dev accuracy: 0.500\n"
          ]
        }
      ],
      "source": [
        "simple_classifier = Classifier(simple_featurize, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "simple_classifier.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ### Understanding the class Classifier\n",
        "def load_data(filename=trainingFile):\n",
        "    data = []\n",
        "    with open(filename, encoding=\"utf8\") as file:\n",
        "        for line in file:\n",
        "            cols = line.split(\"\\t\")\n",
        "            idd = cols[0]\n",
        "            label = cols[1]\n",
        "            text = cols[2]\n",
        "\n",
        "            data.append((idd, label, text))\n",
        "\n",
        "    return data\n",
        "\n",
        "# train_data = load_data()\n",
        "# print(f\"len of train_data: {len(train_data)}\")\n",
        "\n",
        "# ev_data = load_data(filename=evaluationFile)\n",
        "# print(f\"len of load_data: {len(ev_data)}\")\n",
        "\n",
        "test_data = load_data(filename=testFile)\n",
        "print(f\"len of test_data: {len(test_data)}\")\n",
        "\n",
        "\n",
        "# # playing around with nltk.featurize_word:\n",
        "# print(train_data[0][2])\n",
        "# words = nltk.word_tokenize(train_data[0][2])\n",
        "# print(words)\n",
        "\n",
        "test_data[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yOWKeEzm8tG",
        "outputId": "ce62d592-5bc4-4d9e-afe4-da11d43f8af8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len of test_data: 2000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('6678',\n",
              " '',\n",
              " \"Despite some reviews being distinctly Luke-warm, I found the story totally engrossing and even if some critics have described the love story as 'Mills and Boon', so what? It is good to see a warm, touching story of real love in these cynical times. Many in the audience were sniffing and surreptitiously dabbing their eyes. You really believe that the young Victoria and Albert are passionately fond of each other, even though, for political reasons, it was an arranged marriage. I did feel though that Sir John Conroy, who was desperate to control the young Queen, is perhaps played too like a pantomime villain. As it is rumoured that he was in fact, the real father of Victoria (as a result of an affair with her mother The Duchess of Kent) it would have been interesting to explore this theory. Emily Blunt is totally convincing as the young Princess, trapped in the stifling palace with courtiers and politicians out to manipulate her. She brilliantly portrays the strength of character and determination that eventually made Victoria a great Queen of England, which prospered as never before, under her long reign. I believe word of mouth recommendations will ensure great success for this most enjoyable and wonderful looking movie.\\n\")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO4XQzU3PdeU"
      },
      "source": [
        "So we've created a classifier. But is its accuracy score any good?  Let's calculate the accuracy of a \"majority classifier\" to provide some context. This determines the most-represented (majority) class in the training data, and then predicts every test point to be this class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t--LfOjPj7T",
        "outputId": "e70a9196-f7f5-4b2d-a5ed-03fa16686289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Majority class: pos\tDev accuracy: 0.500\n"
          ]
        }
      ],
      "source": [
        "def majority_class(trainY, devY):\n",
        "    labelCounts=Counter()\n",
        "    for label in trainY:\n",
        "        labelCounts[label]+=1\n",
        "    majority_class=labelCounts.most_common(1)[0][0]\n",
        "\n",
        "    correct=0.\n",
        "    for label in devY:\n",
        "        if label == majority_class:\n",
        "            correct+=1\n",
        "\n",
        "    print(\"Majority class: %s\\tDev accuracy: %.3f\" % (majority_class, correct/len(devY)))\n",
        "majority_class(simple_classifier.trainY, simple_classifier.devY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUanyrW9xfRC"
      },
      "source": [
        "The feature we created in `simple_featurize`, evidently, doesn't have a whole lot of legs. In the next portion of the homework, you'll be designing a few features of your own in the hopes of achieving the highest accuracy possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIEkYOWO5ClC"
      },
      "source": [
        "## Deliverable 1: Exploring Features (60 points)\n",
        "\n",
        "Your job in this homework is to implement a binary bag-of-words model (i.e., one that assigns a feature value of 1 to each word type that is present in the text); and to brainstorm **3 additional** distinct classes of features, justify why they might help improve the performance *over a bag of words* for this task, implement them in code, and then assess their independent performance on the development data.\n",
        "\n",
        "To show your work: describe your new features and report their performance in the table below; implement the features in the specified `feature1`, `feature2`, and `feature3` functions, and execute each respective classifier to show its performance. **Double click to edit the table below in markdown mode**.\n",
        "\n",
        "|Feature|Why should it work? (50 words each)|Dev set performance|\n",
        "|---|---|---|\n",
        "|Bag of words| This approach keeps track of the presence of words in a text and provides a dev set accuracy of 0.777. This indicates that keywords matter when it comes to classifying whether a comment is positive or negative which can be done by only looking at the presence of these words and not their spatial relationship in the text | 0.777 ||\n",
        "|Bag of words (frequency of words)| BoW approach with frequency of words in a text produces a comparable dev set performance of 0.768. This indicates that the frequency of words doesn't matter much only their presence does in this context since if a text contains the word 'bad' once or 5 times, there is a chance that the text is a negative comment| 0.768|\n",
        "|Combination of features from AFINN and LIWC|The method calculates valence scores for sentences via the AFINN lexicon, assigning a categorical score ranging from highly negative to positive. LIWC dictionary aids in word-level categorization. Combining all features results in a 0.775 dev set accuracy, comparable to the Bag of Words (BOW) approach. Anticipated superiority over BOW was expected from utilization of an external corpus for text featurization, incorporating supplementary information into the training set.| 0.775 |\n",
        "|n-gram features (3) with AFINN Features|This approach uses the ngram tokenizer from nltk, the output of which is then classified using AFINN lexicon. This set was expected to perform better than BOW since it takes into account the spatial relations of words in a text however, the reduced performance is indicative that classifying positive from negative reviews relies less on positional encodings of the sentence structure and more on the presence of words in a corpus of text| 0.666\n",
        "\n",
        "**You are not graded on your performance, but your justification for why it *should* perform better than a bag of words should be defensible**. Consider the type of data you are working with: what do you look for when writing/reading a movie review?"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JSR_U8g0VZTr"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqKwVYVsxfRC"
      },
      "source": [
        "### Implement Bag-of-Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "vVl1zAREekC3"
      },
      "outputs": [],
      "source": [
        "## A bag-of-words is a representation of text that describes the occurence of words within a document. It involves two things:\n",
        "## - a vocabulary of known words\n",
        "## - a measure of the presence of known words\n",
        "## Any information on the order or structure of words in the document is discarded (similar documents have similar contents)\n",
        "##\n",
        "\n",
        "######################################################################\n",
        "## For `bag_of_words`, please do not use external packages/libraries\n",
        "## Only excepted external libraries are:\n",
        "## - nltk\n",
        "######################################################################\n",
        "\n",
        "def bag_of_words(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name,\n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "\n",
        "    # BEGIN SOLUTION\n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    for word in words:\n",
        "        word=word.lower()\n",
        "\n",
        "        if word not in feats:\n",
        "          feats[word] = 1\n",
        "\n",
        "    # END SOLUTION\n",
        "\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3AJ5qMBeqmL",
        "outputId": "6a215cbc-6589-42d9-e387-bef4832efeed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: bag_of_words, Features: 21078, Train accuracy: 1.000, Dev accuracy: 0.777\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "bow_classifier = Classifier(bag_of_words, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "bow_classifier.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kbt6iGpxfRC"
      },
      "source": [
        "### Implement Your Original Features\n",
        "\n",
        "**Note**: External dictionaries such as [AFINN](https://pypi.org/project/afinn) or [LIWC](https://pypi.org/project/liwc/) might be helpful here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ocPMYhIt4BX0"
      },
      "outputs": [],
      "source": [
        "def feature1(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name,\n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "\n",
        "    # feature1 is a simple frequency based bag-of-words approach where the number of occurences of each word in an example is\n",
        "    # recorded as opposed to the binary bag-of-words approach that simply records the occurence of a word in the whole example.\n",
        "\n",
        "    feats = {}\n",
        "\n",
        "    # BEGIN SOLUTION\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    for word in words:\n",
        "        word=word.lower()\n",
        "\n",
        "        if word not in feats:\n",
        "          feats[word] = 1\n",
        "        else:\n",
        "          feats[word] += 1\n",
        "\n",
        "    # END SOLUTION\n",
        "\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MAwRwbQ7lVw",
        "outputId": "9835b2fe-6cdd-4f9c-8344-2920a3578e7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: feature1, Features: 21078, Train accuracy: 1.000, Dev accuracy: 0.768\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "classifier1 = Classifier(feature1, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "classifier1.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install afinn\n",
        "!pip install -U liwc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkBjNvKA7qsw",
        "outputId": "19996f12-ade7-4038-956f-5b1c920ba16b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting afinn\n",
            "  Downloading afinn-0.1.tar.gz (52 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/52.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m51.2/52.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: afinn\n",
            "  Building wheel for afinn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for afinn: filename=afinn-0.1-py3-none-any.whl size=53429 sha256=30293bd7d170dd28e090f90df96b7459107485f6f586d99da23fad80db70c4cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/05/90/43f79196199a138fb486902fceca30a2d1b5228e6d2db8eb90\n",
            "Successfully built afinn\n",
            "Installing collected packages: afinn\n",
            "Successfully installed afinn-0.1\n",
            "Collecting liwc\n",
            "  Downloading liwc-0.5.0-py2.py3-none-any.whl (5.1 kB)\n",
            "Installing collected packages: liwc\n",
            "Successfully installed liwc-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from afinn import Afinn\n",
        "import liwc\n",
        "path = \"/content/drive/MyDrive/LIWC2007_English100131.dic\"\n",
        "parse, category_names = liwc.load_token_parser(path)\n"
      ],
      "metadata": {
        "id": "N9uplL6s72Rm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LNlQyjEB4Bwt"
      },
      "outputs": [],
      "source": [
        "def feature2(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name,\n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "\n",
        "    feats = {}\n",
        "\n",
        "    # BEGIN SOLUTION\n",
        "    afn = Afinn()\n",
        "    words = nltk.word_tokenize(text)\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    pos, neg, neutral = 0,0,0\n",
        "\n",
        "    overall = afn.score(text) # overall sentiment score from AFINN\n",
        "\n",
        "    for sent in sentences:\n",
        "      score = afn.score(sent)\n",
        "\n",
        "      if score < 0:\n",
        "        neg += 1\n",
        "      elif score > 0:\n",
        "        pos += 1\n",
        "      else:\n",
        "        neutral += 1\n",
        "\n",
        "    text_counts = Counter(category for token in words for category in parse(token))\n",
        "\n",
        "    #feats['AFINN Overall'] = overall\n",
        "    feats['AFINN Positive'] = pos\n",
        "    feats['AFINN Negative'] = neg\n",
        "    feats['AFINN Neutral'] = neutral\n",
        "\n",
        "\n",
        "    for liwc_cat in text_counts.keys():\n",
        "      feats[liwc_cat] = text_counts[liwc_cat]\n",
        "\n",
        "    # END SOLUTION\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgpuykF67oWZ",
        "outputId": "7166e2b0-9489-4c96-a316-2f4d51248cec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: feature2, Features: 67, Train accuracy: 0.779, Dev accuracy: 0.775\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "classifier2 = Classifier(feature2, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "classifier2.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams"
      ],
      "metadata": {
        "id": "28gQdBeaShHy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "FmJKucgn4CEg"
      },
      "outputs": [],
      "source": [
        "def feature3(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name,\n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "\n",
        "    feats = {}\n",
        "    # BEGIN SOLUTION\n",
        "    afn = Afinn()\n",
        "    pos, neg, neutral = 0,0,0\n",
        "    sent_pos, sent_neg, sent_neutral = 0,0,0\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    trigrams = ngrams(words,3)\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "    for sent in sentences:\n",
        "      score = afn.score(sent)\n",
        "\n",
        "      if score > 0:\n",
        "        sent_pos += 1\n",
        "      elif score < 0:\n",
        "        sent_neg += 1\n",
        "      else:\n",
        "        sent_neutral += 1\n",
        "\n",
        "    feats['AFINN Positive'] = pos\n",
        "    feats['AFINN Negative'] = neg\n",
        "    feats['AFINN Neutral'] = neutral\n",
        "\n",
        "    for checking in trigrams:\n",
        "      score = afn.score(''.join(checking)) # overall sentiment score from AFINN\n",
        "\n",
        "      if score > 0:\n",
        "        pos += 1\n",
        "      elif score < 0:\n",
        "        neg += 1\n",
        "      else:\n",
        "        neutral += 1\n",
        "\n",
        "    feats['AFINN Positive ngram'] = pos\n",
        "    feats['AFINN Negative ngram'] = neg\n",
        "    feats['AFINN Neutral ngram'] = neutral\n",
        "\n",
        "    # END SOLUTION\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_f--utb7q4l",
        "outputId": "4340ff49-357f-4162-ffe0-0d8011b39983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: feature3, Features: 6, Train accuracy: 0.646, Dev accuracy: 0.666\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "classifier3 = Classifier(feature3, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "classifier3.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J6xbUaV5ZYOi"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg2J1BLgatMP"
      },
      "source": [
        "## Deliverable 2: Final Classifier (20 points)\n",
        "\n",
        "The two cells in \"Combine your features\" will generate a file named `combiner_function_predictions.csv`. **Download this file (using e.g. the file manager on the left panel in Colab) and submit this to GradeScope along with your notebook**\n",
        "\n",
        "Your score is computed by the following equation: `min(20.0, 200 * max(test_accuracy - 0.69, 0.0))`. **That is, you receive full credits with test accuracy >= 0.79 and no credits with test accuracy <= 0.69**.\n",
        "\n",
        "The 5 students with the highest performance (revealed after the submission deadline) will receive extra credit for this assignment.\n",
        "\n",
        "Please do not change the auto-generated filename!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWwjhLnMxfRD"
      },
      "source": [
        "### Combine your features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEpK5LyMgv5c"
      },
      "source": [
        "Next, let's combine any or all the features you have developed into one big model and make predictions on the test data. There is no exact number/threshold we're looking for, accuracy-wise, but the combiner function should *generally* have a higher accuracy than BoW on its own (assuming your features are adding additional information beyond what BoW is adding).\n",
        "\n",
        "You don't need to edit the following cell, unless you want to change which features are handed off to the \"combined\" model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "jxKmEqI5JY71"
      },
      "outputs": [],
      "source": [
        "def combiner_function(text):\n",
        "\n",
        "    # Here the `all_feats` dict should contain the features -- the key should be the feature name,\n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    # at the moment, all 4 of: bag of words and your 3 original features are handed off to the combined model\n",
        "    # update the values within [bag_of_words, feature1, feature2, feature3] to change this.\n",
        "\n",
        "    all_feats={}\n",
        "    for feature in [bag_of_words, feature1, feature2, feature3]:\n",
        "        all_feats.update(feature(text))\n",
        "    return all_feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-tRUFTIdAqT",
        "outputId": "8a91cb09-d543-4791-b38e-f8336b3f27c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: combiner_function, Features: 21111, Train accuracy: 1.000, Dev accuracy: 0.801\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "big_classifier = Classifier(combiner_function, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "big_classifier.evaluate()\n",
        "\n",
        "#generate .csv file with prediction output on test data\n",
        "big_classifier.predict()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "94EFoP82WYCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNFfc0CexfRD"
      },
      "source": [
        "## Deliverable 3: Analyze and Tune Your Classifier (5 points)\n",
        "\n",
        "\n",
        "Below you will find several ways in which you can interrogate your model to get ideas on ways to improve its performance. **Section 1 to 3 are for exploration. The only deliverable is in section 4.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fRqlvAyxfRD"
      },
      "source": [
        "### 1. Confusion Matrix\n",
        "\n",
        "First, let's look at the confusion matrix of its predictions (where we can compare the true labels with the predicted labels). What kinds of mistakes is it making? (While this is mainly helpful in the context of multiclass classification, we can still see if there's a bias toward predicting a specific class in the binary setting as well). **No written response required.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "7ulxd1TosIMV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "outputId": "9778d030-95ec-41d2-ab25-899b2d0fc8e4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def print_confusion(classifier):\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "\n",
        "    # Compute the confusion matrix\n",
        "    y_pred = classifier.log_reg.predict(classifier.devX)\n",
        "    cm = confusion_matrix(classifier.devY, y_pred)\n",
        "\n",
        "    # Use ConfusionMatrixDisplay to plot\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    disp.plot(ax=ax, xticks_rotation=\"vertical\", values_format=\"d\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "print_confusion(big_classifier)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPhH4flIuEbx"
      },
      "source": [
        "### 2. Feature Weights\n",
        "\n",
        "Next, let's look at the features that are most important for each of the classes (ranked by how strong their corresponding coefficient is). Do the features you are defining help in the ways you think they should?  Do sets of successful features suggests others, or complementary features that may provide a different view on the data? **No written response required.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "IAyGuXIi9pqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "799f2c8c-78d9-419f-e3b2-cb08c355baa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos\t0.520\tman\n",
            "pos\t0.520\tnew\n",
            "pos\t0.520\tamazing\n",
            "pos\t0.516\tseries\n",
            "pos\t0.501\tgreat\n",
            "pos\t0.460\tperformance\n",
            "pos\t0.449\texcellent\n",
            "pos\t0.443\twill\n",
            "pos\t0.425\tviolence\n",
            "pos\t0.422\tget\n",
            "pos\t0.411\tenjoyed\n",
            "pos\t0.408\ttwo\n",
            "pos\t0.396\tother\n",
            "pos\t0.394\tthought\n",
            "pos\t0.394\tprobably\n",
            "pos\t0.390\tsome\n",
            "pos\t0.385\ttop\n",
            "pos\t0.383\taway\n",
            "pos\t0.382\tperfect\n",
            "pos\t0.382\tseen\n",
            "pos\t0.367\trelationship\n",
            "pos\t0.366\tit\n",
            "pos\t0.358\tamerican\n",
            "pos\t0.354\tgone\n",
            "pos\t0.348\thim\n",
            "\n",
            "neg\t-0.741\tworst\n",
            "neg\t-0.649\tacting\n",
            "neg\t-0.588\tidea\n",
            "neg\t-0.571\tnegemo\n",
            "neg\t-0.567\twatching\n",
            "neg\t-0.515\tactors\n",
            "neg\t-0.483\tinstead\n",
            "neg\t-0.476\twaste\n",
            "neg\t-0.464\tboring\n",
            "neg\t-0.451\t?\n",
            "neg\t-0.450\tthere\n",
            "neg\t-0.438\tpretty\n",
            "neg\t-0.434\tperformances\n",
            "neg\t-0.432\t're\n",
            "neg\t-0.419\tnothing\n",
            "neg\t-0.414\ttv\n",
            "neg\t-0.407\thalf\n",
            "neg\t-0.398\tpoor\n",
            "neg\t-0.396\tmany\n",
            "neg\t-0.391\tworse\n",
            "neg\t-0.390\tscript\n",
            "neg\t-0.390\tsilly\n",
            "neg\t-0.390\tthan\n",
            "neg\t-0.382\tsay\n",
            "neg\t-0.368\tstupid\n",
            "\n"
          ]
        }
      ],
      "source": [
        "big_classifier.printWeights(n=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e80DUsSXu7h9"
      },
      "source": [
        "### 3. Misclassified Data Points\n",
        "\n",
        "Next, let's look at the individual data points that are the hardest to classify correctly. Does it suggest any features you might create to disentangle them? **No written response required.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "I4uTzwV99pqe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "71770c13-c789-4907-84a9-7a97582c365f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      id  P(predicted class confidence) Human label Prediction  \\\n",
              "0   1178                       1.000000         pos        neg   \n",
              "1   1889                       0.999998         pos        neg   \n",
              "2   1659                       0.999977         pos        neg   \n",
              "3   1608                       0.999972         neg        pos   \n",
              "4   1247                       0.999955         neg        pos   \n",
              "5   1004                       0.999943         neg        pos   \n",
              "6   1261                       0.999931         neg        pos   \n",
              "7   1337                       0.999923         pos        neg   \n",
              "8   1436                       0.999919         pos        neg   \n",
              "9   1896                       0.999826         neg        pos   \n",
              "10  1139                       0.999726         pos        neg   \n",
              "11  1508                       0.999679         pos        neg   \n",
              "12  1330                       0.999523         pos        neg   \n",
              "13  1218                       0.999518         neg        pos   \n",
              "14  1646                       0.999501         neg        pos   \n",
              "15  1822                       0.999488         pos        neg   \n",
              "16  1110                       0.999351         neg        pos   \n",
              "17  1931                       0.999267         pos        neg   \n",
              "18  1309                       0.999229         neg        pos   \n",
              "19  1531                       0.998819         neg        pos   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                               Text  \n",
              "0   While I do not think this was a perfect 10, I do agree it was way above a 6 which is what it's rated here. No, Brokedown Palace was not perfect and yes it's plot has been done many times before. That doesn't mean it shouldn't be done again if it is done well and I think this movie had some strong moments. The acting of Claire Danes, as already mentioned many times, was flawless as was Kate Bec...  \n",
              "1   At times, this overtakes The Thing as my favourite horror film. While Carpenter's film is the more efficient and more entertaining flick, Kubrick's is more artistic, more thought-provoking, and probably scarier. It's one of the few films where I can look past its flaws and truly and wholly love it. I try not to compare it to the book  which I've only read once, a number of years ago, and whic...  \n",
              "2   Child 'Sexploitation' is one of the most serious issues facing our world today and I feared that any film on the topic would jump straight to scenes of an explicitly sexual nature in order to shock and disturb the audience. After having seen both 'Trade' and 'Holly', one film moved me to want to actually see a change in international laws. The other felt like a poor attempt at making me cry fo...  \n",
              "3   As good an advert for republicanism as you're ever likely to see,\"Mayerling\"is an everyday story of royal folk in late nineteenth century Austria.Set during one of Europe's seemingly incessant internal turmoils it concerns itself with the Emperor Franz Joseph (Mr James Mason),his rebellious son,the Crown Prince Rudolf (Mr Omar Sharif)the Empress(Miss Ava Gardner) and various mistresses,secret ...  \n",
              "4   Let's start by the simple lines. From the viewer's side, there a couple of good \"director details\", some points of view at the movie scenes that are nice. The special effects are good enough, a good acting/good scenery also. But the story is way too simple. It shows how a elite Army bomb squad unit lives, acts and sometimes dies. It shows the drama of living in war. In my movie experience as a...  \n",
              "5   Certainly NOMAD has some of the best horse riding scenes, swordplay, and scrumptious landscape cinematography you'll likely see, but this isn't what makes a film good. It helps but the story has to shine through on top of these things. And that's where Nomad wanders.The story is stilted, giving it a sense that it was thrown together simply to make a \"cool\" movie that \"looks\" great. Not to ment...  \n",
              "6   NATURAL BORN KILLERS (1994)Cinema Cut: R Director's Cut: NC-17It's an unusual Oliver Stone picture, but when I read he was on drugs during the filming, I needed no further explanation. 'Natural Born Killers' is a risky, mad, all out film-making that we do not get very often; strange, psychotic, artistic pictures.'Natural Born Killers' is basically the story of how two mass killers were popular...  \n",
              "7   FULL OF SPOILERS.This is a pretty fast and enjoyable crime thriller based on Ira Levin's play about two gay playwrights (Caine and Reeve) that plot the murder of one's rich wife (Cannon) to get the property and the insurance. The plot succeeds but Christopher Reeve as the younger and less established of the two writers decides to make a play out of the actual murder -- with only slight changes...  \n",
              "8   I have never seen such a movie before. I was on the edge of my seat and constantly laughing throughout the entire movie. I never thought such horrible acting existed it was all just too funny. The story behind the movie is decent but the movies scenes fail to portray them. I have never seen such a stupid movie in my life which is why it I think its worth watching. I give this movie 10 out of 1...  \n",
              "9   STAR RATING: ***** The Works **** Just Misses the Mark *** That Little Bit In Between ** Lagging Behind * The Pits In this debut effort for Nick Park's beloved man and dog, they are forced to fly to the moon when good old Wallace runs out of cheese.As well as being the shortest feature at just 22 minutes, this W/G adventure is also the earliest and it kinda shows. The plasticine animation is a...  \n",
              "10  I saw this movie as a kid on Creature Feature when I lived in New York. It was a pretty creepy movie, though not as good as Horror Hotel. I just bought this movie on DVD, and it is different from what I remember because in the DVD that I bought there are several scenes where the actors speak in French and/or Italian and no subtitles are provided. Then the other actors respond in English to wha...  \n",
              "11  Gruveyman2 (comment below)you are a complete idiot...blinded by ignorance by the very city you have allegiance to. Its that whiny arrogance, that you are ironically claiming the film exudes about SF, that makes you seem like such the typical LA A**hole! The only reason you felt the film was so self congratulatory about SF is because you are jealous. Of course you don't know it because you are ...  \n",
              "12  Ride With the Devil has something rich and special, if you can stand the slow development. While tackling a dark, gritty subject, the brutal guerrilla war in the American West during its Civil War (which in turned spawned the outlaws of the old west of the 1870s), the movie maintains a strangely satisfying, unmanipulated atmosphere. What I'm refering to is the tendency of films' music and ligh...  \n",
              "13  This early B entry into the patriotic category slapped a gorgeous young Gene Tierney on the ads and posters, but you have to wait a good time before you glimpse her, riding in a Hollywoodized camel train. Previously, we've set up George Sanders and Bruce Cabot in the desert as guys who barely get along, but must rally in the face of attack. I've seen Sanders as so many enjoyable cads that it w...  \n",
              "14  Anyone who visited drive-ins in the 1950s, 60s, and 70s, must have seen a film or two by American International Pictures, a distributor that resembled 1980s giant Cannon Films. Wherever movie-goers ventured, AIP would be right there to supply the latest en vogue titles - in the 50s came horror movies like 'Voodoo Woman' and 'The Undead;' in the 60s were Frankie Avalon-Annette Funicello beach c...  \n",
              "15  Zombi 3 has an interesting history in it's making. Firstly, it is a sequel to Fulci's hit Zombi 2, with Zombi 2 itself being of course a marketing ploy to trick people into thinking it was a sequel to George A. Romero's Dawn of the Dead aka Zombi. Confusing enough? Basically, none of the films have anything to do with one another, but who cares when they make money. I guess Fulci himself start...  \n",
              "16  Before Stan Laurel became the smaller half of the all-time greatest comedy team, he laboured under contract to Broncho Billy Anderson in a series of cheapies, many of which were parodies of major Hollywood features. Following a dispute with Anderson, Laurel continued the informal series of parodies at Joe Rock's smaller (and more indigent) production company.Most of Laurel's parody films were ...  \n",
              "17  She's not Michael JordanThink of all the marvelous NBA players whose career light has been dimmed for no other good reason than the timing of their birth. Their names might trip as easily off the public's lips as Russell, Cousy, Byrd, Magic, McHale, Oscar, Wilt, the list goes on; but the volume gets turned down a little, for Stockton and Malone, Charles, Patrick and all the other great players...  \n",
              "18  Scooby Doo is undoubtedly one of the most simple, successful and beloved cartoon characters in the world. So, what happens when you've been everywhere and done everything with the formula? You switch it up right? Wrong. You stop production and let it rest for a decade or so and then run it again, keeping the core of its success intact. That is to say, stick with the formula for the most part b...  \n",
              "19  ***spoilers***spoilers***spoilers***spoilersThere are bad movies and then there are movies which are so awful that they become affectionately comical in their ineptness. Such is the case with Columbia Pictures' 'The Grudge.' This cinematic atrocity began when an otherwise well intentioned American saw a Japanese made for TV film 'Ju-on' and was inspired to remake the movie in English. This beg...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9d5f5216-4b90-4dee-934d-1f85e2eb57e8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>P(predicted class confidence)</th>\n",
              "      <th>Human label</th>\n",
              "      <th>Prediction</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1178</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>While I do not think this was a perfect 10, I do agree it was way above a 6 which is what it's rated here. No, Brokedown Palace was not perfect and yes it's plot has been done many times before. That doesn't mean it shouldn't be done again if it is done well and I think this movie had some strong moments. The acting of Claire Danes, as already mentioned many times, was flawless as was Kate Bec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1889</td>\n",
              "      <td>0.999998</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>At times, this overtakes The Thing as my favourite horror film. While Carpenter's film is the more efficient and more entertaining flick, Kubrick's is more artistic, more thought-provoking, and probably scarier. It's one of the few films where I can look past its flaws and truly and wholly love it. I try not to compare it to the book  which I've only read once, a number of years ago, and whic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1659</td>\n",
              "      <td>0.999977</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>Child 'Sexploitation' is one of the most serious issues facing our world today and I feared that any film on the topic would jump straight to scenes of an explicitly sexual nature in order to shock and disturb the audience. After having seen both 'Trade' and 'Holly', one film moved me to want to actually see a change in international laws. The other felt like a poor attempt at making me cry fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1608</td>\n",
              "      <td>0.999972</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>As good an advert for republicanism as you're ever likely to see,\"Mayerling\"is an everyday story of royal folk in late nineteenth century Austria.Set during one of Europe's seemingly incessant internal turmoils it concerns itself with the Emperor Franz Joseph (Mr James Mason),his rebellious son,the Crown Prince Rudolf (Mr Omar Sharif)the Empress(Miss Ava Gardner) and various mistresses,secret ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1247</td>\n",
              "      <td>0.999955</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Let's start by the simple lines. From the viewer's side, there a couple of good \"director details\", some points of view at the movie scenes that are nice. The special effects are good enough, a good acting/good scenery also. But the story is way too simple. It shows how a elite Army bomb squad unit lives, acts and sometimes dies. It shows the drama of living in war. In my movie experience as a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1004</td>\n",
              "      <td>0.999943</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Certainly NOMAD has some of the best horse riding scenes, swordplay, and scrumptious landscape cinematography you'll likely see, but this isn't what makes a film good. It helps but the story has to shine through on top of these things. And that's where Nomad wanders.The story is stilted, giving it a sense that it was thrown together simply to make a \"cool\" movie that \"looks\" great. Not to ment...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1261</td>\n",
              "      <td>0.999931</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>NATURAL BORN KILLERS (1994)Cinema Cut: R Director's Cut: NC-17It's an unusual Oliver Stone picture, but when I read he was on drugs during the filming, I needed no further explanation. 'Natural Born Killers' is a risky, mad, all out film-making that we do not get very often; strange, psychotic, artistic pictures.'Natural Born Killers' is basically the story of how two mass killers were popular...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1337</td>\n",
              "      <td>0.999923</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>FULL OF SPOILERS.This is a pretty fast and enjoyable crime thriller based on Ira Levin's play about two gay playwrights (Caine and Reeve) that plot the murder of one's rich wife (Cannon) to get the property and the insurance. The plot succeeds but Christopher Reeve as the younger and less established of the two writers decides to make a play out of the actual murder -- with only slight changes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1436</td>\n",
              "      <td>0.999919</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>I have never seen such a movie before. I was on the edge of my seat and constantly laughing throughout the entire movie. I never thought such horrible acting existed it was all just too funny. The story behind the movie is decent but the movies scenes fail to portray them. I have never seen such a stupid movie in my life which is why it I think its worth watching. I give this movie 10 out of 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1896</td>\n",
              "      <td>0.999826</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>STAR RATING: ***** The Works **** Just Misses the Mark *** That Little Bit In Between ** Lagging Behind * The Pits In this debut effort for Nick Park's beloved man and dog, they are forced to fly to the moon when good old Wallace runs out of cheese.As well as being the shortest feature at just 22 minutes, this W/G adventure is also the earliest and it kinda shows. The plasticine animation is a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1139</td>\n",
              "      <td>0.999726</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>I saw this movie as a kid on Creature Feature when I lived in New York. It was a pretty creepy movie, though not as good as Horror Hotel. I just bought this movie on DVD, and it is different from what I remember because in the DVD that I bought there are several scenes where the actors speak in French and/or Italian and no subtitles are provided. Then the other actors respond in English to wha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1508</td>\n",
              "      <td>0.999679</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>Gruveyman2 (comment below)you are a complete idiot...blinded by ignorance by the very city you have allegiance to. Its that whiny arrogance, that you are ironically claiming the film exudes about SF, that makes you seem like such the typical LA A**hole! The only reason you felt the film was so self congratulatory about SF is because you are jealous. Of course you don't know it because you are ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1330</td>\n",
              "      <td>0.999523</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>Ride With the Devil has something rich and special, if you can stand the slow development. While tackling a dark, gritty subject, the brutal guerrilla war in the American West during its Civil War (which in turned spawned the outlaws of the old west of the 1870s), the movie maintains a strangely satisfying, unmanipulated atmosphere. What I'm refering to is the tendency of films' music and ligh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1218</td>\n",
              "      <td>0.999518</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>This early B entry into the patriotic category slapped a gorgeous young Gene Tierney on the ads and posters, but you have to wait a good time before you glimpse her, riding in a Hollywoodized camel train. Previously, we've set up George Sanders and Bruce Cabot in the desert as guys who barely get along, but must rally in the face of attack. I've seen Sanders as so many enjoyable cads that it w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1646</td>\n",
              "      <td>0.999501</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Anyone who visited drive-ins in the 1950s, 60s, and 70s, must have seen a film or two by American International Pictures, a distributor that resembled 1980s giant Cannon Films. Wherever movie-goers ventured, AIP would be right there to supply the latest en vogue titles - in the 50s came horror movies like 'Voodoo Woman' and 'The Undead;' in the 60s were Frankie Avalon-Annette Funicello beach c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1822</td>\n",
              "      <td>0.999488</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>Zombi 3 has an interesting history in it's making. Firstly, it is a sequel to Fulci's hit Zombi 2, with Zombi 2 itself being of course a marketing ploy to trick people into thinking it was a sequel to George A. Romero's Dawn of the Dead aka Zombi. Confusing enough? Basically, none of the films have anything to do with one another, but who cares when they make money. I guess Fulci himself start...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1110</td>\n",
              "      <td>0.999351</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Before Stan Laurel became the smaller half of the all-time greatest comedy team, he laboured under contract to Broncho Billy Anderson in a series of cheapies, many of which were parodies of major Hollywood features. Following a dispute with Anderson, Laurel continued the informal series of parodies at Joe Rock's smaller (and more indigent) production company.Most of Laurel's parody films were ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1931</td>\n",
              "      <td>0.999267</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>She's not Michael JordanThink of all the marvelous NBA players whose career light has been dimmed for no other good reason than the timing of their birth. Their names might trip as easily off the public's lips as Russell, Cousy, Byrd, Magic, McHale, Oscar, Wilt, the list goes on; but the volume gets turned down a little, for Stockton and Malone, Charles, Patrick and all the other great players...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1309</td>\n",
              "      <td>0.999229</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Scooby Doo is undoubtedly one of the most simple, successful and beloved cartoon characters in the world. So, what happens when you've been everywhere and done everything with the formula? You switch it up right? Wrong. You stop production and let it rest for a decade or so and then run it again, keeping the core of its success intact. That is to say, stick with the formula for the most part b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1531</td>\n",
              "      <td>0.998819</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>***spoilers***spoilers***spoilers***spoilersThere are bad movies and then there are movies which are so awful that they become affectionately comical in their ineptness. Such is the case with Columbia Pictures' 'The Grudge.' This cinematic atrocity began when an otherwise well intentioned American saw a Japanese made for TV film 'Ju-on' and was inspired to remake the movie in English. This beg...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d5f5216-4b90-4dee-934d-1f85e2eb57e8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9d5f5216-4b90-4dee-934d-1f85e2eb57e8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9d5f5216-4b90-4dee-934d-1f85e2eb57e8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d527a81c-d8e4-4804-9dc8-1845263f14d9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d527a81c-d8e4-4804-9dc8-1845263f14d9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d527a81c-d8e4-4804-9dc8-1845263f14d9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def analyze(classifier):\n",
        "\n",
        "    probs=classifier.log_reg.predict_proba(classifier.devX)\n",
        "    predicts=classifier.log_reg.predict(classifier.devX)\n",
        "\n",
        "    classes={}\n",
        "    for idx, lab in enumerate(classifier.log_reg.classes_):\n",
        "        classes[lab]=idx\n",
        "\n",
        "    mistakes={}\n",
        "    for i in range(len(probs)):\n",
        "        if predicts[i] != classifier.devY[i]:\n",
        "            predicted_lab_idx=classes[predicts[i]]\n",
        "            mistakes[i]=probs[i][predicted_lab_idx]\n",
        "\n",
        "    frame=[]\n",
        "    sorted_x = sorted(mistakes.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    for k, v in sorted_x:\n",
        "        idd=classifier.devOrig[k][0]\n",
        "        text=classifier.devOrig[k][2]\n",
        "        frame.append([idd, v, classifier.devY[k], predicts[k], text])\n",
        "\n",
        "    df=pd.DataFrame(frame, columns=[\"id\", \"P(predicted class confidence)\", \"Human label\", \"Prediction\", \"Text\"])\n",
        "\n",
        "    with option_context('display.max_colwidth', 400):\n",
        "        display(df.head(n=20))\n",
        "\n",
        "analyze(big_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_rqigrjxfRH"
      },
      "source": [
        "### 4. Hyperparameter Tuning\n",
        "\n",
        "An important step in optimizing models is hyperparameter tuning. Hyperparameter tuning enables us to adjust the hyperparameters of our model, specifically those not learned from the data, to optimize its performance. Below, you will use K-fold cross-validation on the training data to get an estimate of your model's performance. For this deliverable, **replace `classifier` with the classifier you want to improve. Then, complete `param_grid` for `C` (values closer to 0 = stronger regularization)** to find which L2 regularization strength improves your model the most. For more information, please consult [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
        "\n",
        "*Note: we are unable to hyperparameter tune `min_feature_size` easily with the scikitlearn API (we hard coded `min_feature_size` in the `Classifier` class). Please try out different values manually.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "FIYOH5JPxfRH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "376b1bf5-1e14-4bd3-a25e-99aa221afd40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'C': 1}\n"
          ]
        }
      ],
      "source": [
        "# import libraries\n",
        "from sklearn.model_selection import KFold, GridSearchCV\n",
        "\n",
        "# Parameter Grid for L2 regularization strength\n",
        "# BEGIN SOLUTION\n",
        "param_grid = {'C': [0.01,1,5]}\n",
        "# END SOLUTION\n",
        "\n",
        "# initialize KFold object\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "#classifier = feature2 # TODO: Change to the classifier you want to hyperparameter tune\n",
        "classifier = Classifier(bag_of_words, L2_regularization_strength=1.0, min_feature_count=1) # hyper-parameter tuning on the second feature\n",
        "\n",
        "grid_search = GridSearchCV(estimator= classifier.log_reg, param_grid=param_grid, scoring='accuracy', cv=kf)\n",
        "grid_search.fit(classifier.trainX, classifier.trainY)\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Which parameters performed best?\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeMFRYz5fQWx",
        "outputId": "79d3e83f-bb23-45c6-fcb0-241807c344f7"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCguu4DlxfRH"
      },
      "source": [
        "## Deliverable 4: Short Answer (15 points)\n",
        "\n",
        "Give a real-world example of where it is important to capture the percentage of false positives the model predicts and another example of a case where it is important to capture false negatives. Why might these measures be more valuable than finding the model's accuracy? (~50 words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owUsgFg5xfRH"
      },
      "source": [
        "While the accuracy of a machine learning model usually holds substance, it is important to dive deeper into metrics beyond the ratio of correct/total predictions. This is where terms like false positive, true positive, false negative, true negative come into play which are then used to compute the precision, recall, and f-1 score which gives a better understanding of the model performance.\n",
        "\n",
        "There may be instances where it might be important to capture false negatives (predicted class is negative but actual class is positive). The obvious example in this case is a preliminary cancer screening test and, a false negative means a person who has cancer is not diagnosed initially when intervention could have potentially saved the person's life early onwards.\n",
        "\n",
        "There may be instances where it might be important to capture false positives (predicted class is positive but actual class is negative). The obvious example in this case could be a spam-ham classifier where, a false positive could mean an urgent email was not spam but it got sent to the spam folder.\n",
        "\n",
        "One way of tackling with this problem is to introduce asymmetric loss functions during optimization."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WmGDXcc9g_yR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}